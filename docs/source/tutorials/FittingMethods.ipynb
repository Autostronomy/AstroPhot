{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Methods\n",
    "\n",
    "Here we will explore the various fitting methods in AstroPhot. You have already encountered some of the methods, but here we will take a more systematic approach and discuss their strengths/weaknesses. Each method will be applied to the same problem with the same initial conditions so you can see how they operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import gaussian_kde as kde\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import astrophot as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup a fitting problem. You can ignore this cell to start, it just makes some test data to fit\n",
    "\n",
    "\n",
    "def true_params():\n",
    "\n",
    "    # just some random parameters to use for fitting. Feel free to play around with these to see what happens!\n",
    "    sky_param = np.array([10**1.5])\n",
    "    sersic_params = np.array(\n",
    "        [\n",
    "            [\n",
    "                58.44035491,\n",
    "                55.58516735,\n",
    "                0.54945988,\n",
    "                37.19794926 * np.pi / 180,\n",
    "                2.14513004,\n",
    "                22.05219055,\n",
    "                10**2.45583024,\n",
    "            ],\n",
    "            [\n",
    "                44.00353786,\n",
    "                31.54430634,\n",
    "                0.40203928,\n",
    "                172.03862521 * np.pi / 180,\n",
    "                2.88613347,\n",
    "                12.095631,\n",
    "                10**2.76711163,\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return sersic_params, sky_param\n",
    "\n",
    "\n",
    "def init_params():\n",
    "\n",
    "    sky_param = np.array([10**1.4])\n",
    "    sersic_params = np.array(\n",
    "        [\n",
    "            [57.0, 56.0, 0.6, 40.0 * np.pi / 180, 1.5, 25.0, 10**2.0],\n",
    "            [45.0, 30.0, 0.5, 170.0 * np.pi / 180, 2.0, 10.0, 10**3.0],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return sersic_params, sky_param\n",
    "\n",
    "\n",
    "def initialize_model(target, use_true_params=True):\n",
    "\n",
    "    # Pick parameters to start the model with\n",
    "    if use_true_params:\n",
    "        sersic_params, sky_param = true_params()\n",
    "    else:\n",
    "        sersic_params, sky_param = init_params()\n",
    "\n",
    "    # List of models, starting with the sky\n",
    "    model_list = [\n",
    "        ap.Model(\n",
    "            name=\"sky\",\n",
    "            model_type=\"flat sky model\",\n",
    "            target=target,\n",
    "            I=sky_param[0],\n",
    "        )\n",
    "    ]\n",
    "    # Add models to the list\n",
    "    for i, params in enumerate(sersic_params):\n",
    "        model_list.append(\n",
    "            ap.Model(\n",
    "                name=f\"sersic {i}\",\n",
    "                model_type=\"sersic galaxy model\",\n",
    "                target=target,\n",
    "                center=[params[0], params[1]],\n",
    "                q=params[2],\n",
    "                PA=params[3],\n",
    "                n=params[4],\n",
    "                Re=params[5],\n",
    "                Ie=params[6],\n",
    "                # psf_convolve = True, # uncomment to try everything with PSF blurring (takes longer)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    MODEL = ap.Model(\n",
    "        name=\"group\",\n",
    "        model_type=\"group model\",\n",
    "        models=model_list,\n",
    "        target=target,\n",
    "    )\n",
    "    # Make sure every model is ready to go\n",
    "    MODEL.initialize()\n",
    "\n",
    "    return MODEL\n",
    "\n",
    "\n",
    "def generate_target():\n",
    "\n",
    "    N = 99\n",
    "    pixelscale = 1.0\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # PSF has sigma of 2x pixelscale\n",
    "    PSF = ap.utils.initialize.gaussian_psf(2, 21, pixelscale)\n",
    "    PSF /= np.sum(PSF)\n",
    "\n",
    "    target = ap.TargetImage(\n",
    "        data=np.zeros((N, N)),\n",
    "        pixelscale=pixelscale,\n",
    "        psf=PSF,\n",
    "    )\n",
    "\n",
    "    MODEL = initialize_model(target, True)\n",
    "\n",
    "    # Sample the model with the true values to make a mock image\n",
    "    img = MODEL().data.T.detach().cpu().numpy()\n",
    "    # Add poisson noise\n",
    "    target.data = torch.Tensor(img + rng.normal(scale=np.sqrt(img) / 2))\n",
    "    target.variance = torch.Tensor(img / 4)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ap.plots.target_image(fig, ax, target)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def corner_plot(\n",
    "    chain,\n",
    "    labels=None,\n",
    "    bins=None,\n",
    "    true_values=None,\n",
    "    plot_density=True,\n",
    "    plot_contours=True,\n",
    "    figsize=(10, 10),\n",
    "):\n",
    "    ndim = chain.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim, ndim, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "    if bins is None:\n",
    "        bins = int(np.sqrt(chain.shape[0]))\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            i_range = (np.min(chain[:, i]), np.max(chain[:, i]))\n",
    "            j_range = (np.min(chain[:, j]), np.max(chain[:, j]))\n",
    "            if i == j:\n",
    "                # Plot the histogram of parameter i\n",
    "                # ax.hist(chain[:, i], bins=bins, histtype=\"step\", range = i_range, density=True, color=\"k\", lw=1)\n",
    "\n",
    "                if plot_density:\n",
    "                    # Plot the kernel density estimate\n",
    "                    kde_x = np.linspace(i_range[0], i_range[1], 100)\n",
    "                    kde_y = kde(chain[:, i])(kde_x)\n",
    "                    ax.plot(kde_x, kde_y, color=\"green\", lw=1)\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                ax.set_xlim(i_range)\n",
    "\n",
    "            elif i > j:\n",
    "                # Plot the 2D histogram of parameters i and j\n",
    "                # ax.hist2d(chain[:, j], chain[:, i], bins=bins, cmap=\"Greys\")\n",
    "\n",
    "                if plot_contours:\n",
    "                    # Plot the kernel density estimate contours\n",
    "                    kde_ij = kde([chain[:, j], chain[:, i]])\n",
    "                    x, y = np.mgrid[j_range[0] : j_range[1] : 100j, i_range[0] : i_range[1] : 100j]\n",
    "                    positions = np.vstack([x.ravel(), y.ravel()])\n",
    "                    kde_pos = np.reshape(kde_ij(positions).T, x.shape)\n",
    "                    ax.contour(x, y, kde_pos, colors=\"green\", linewidths=1, levels=3)\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[j], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                    ax.axhline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                ax.set_xlim(j_range)\n",
    "                ax.set_ylim(i_range)\n",
    "\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            if j == 0 and labels is not None:\n",
    "                ax.set_ylabel(labels[i])\n",
    "            ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "            if i == ndim - 1 and labels is not None:\n",
    "                ax.set_xlabel(labels[j])\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def corner_plot_covariance(\n",
    "    cov_matrix, mean, labels=None, figsize=(10, 10), true_values=None, ellipse_colors=\"g\"\n",
    "):\n",
    "    num_params = cov_matrix.shape[0]\n",
    "    fig, axes = plt.subplots(num_params, num_params, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "\n",
    "    for i in range(num_params):\n",
    "        for j in range(num_params):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                x = np.linspace(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    100,\n",
    "                )\n",
    "                y = norm.pdf(x, mean[i], np.sqrt(cov_matrix[i, i]))\n",
    "                ax.plot(x, y, color=\"g\")\n",
    "                ax.set_xlim(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]), mean[i] + 3 * np.sqrt(cov_matrix[i, i])\n",
    "                )\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "            elif j < i:\n",
    "                cov = cov_matrix[np.ix_([j, i], [j, i])]\n",
    "                lambda_, v = np.linalg.eig(cov)\n",
    "                lambda_ = np.sqrt(lambda_)\n",
    "                angle = np.rad2deg(np.arctan2(v[1, 0], v[0, 0]))\n",
    "                for k in [1, 2]:\n",
    "                    ellipse = Ellipse(\n",
    "                        xy=(mean[j], mean[i]),\n",
    "                        width=lambda_[0] * k * 2,\n",
    "                        height=lambda_[1] * k * 2,\n",
    "                        angle=angle,\n",
    "                        edgecolor=ellipse_colors,\n",
    "                        facecolor=\"none\",\n",
    "                    )\n",
    "                    ax.add_artist(ellipse)\n",
    "\n",
    "                # Set axis limits\n",
    "                margin = 3\n",
    "                ax.set_xlim(\n",
    "                    mean[j] - margin * np.sqrt(cov_matrix[j, j]),\n",
    "                    mean[j] + margin * np.sqrt(cov_matrix[j, j]),\n",
    "                )\n",
    "                ax.set_ylim(\n",
    "                    mean[i] - margin * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + margin * np.sqrt(cov_matrix[i, i]),\n",
    "                )\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[j], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                    ax.axhline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "\n",
    "            if j > i:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            if i < num_params - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_xlabel(labels[j])\n",
    "            ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "target = generate_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt\n",
    "\n",
    "This fitter is identitied as `ap.fit.LM` and it employs a variant of the second order Newton's method to converge very quickly to the local minimum. This is the generally accepted best algorithm for most use cases in $\\chi^2$ minimization. If you don't know what to pick, start with this minimizer. The LM optimizer bridges the gap between first-order gradient descent and second order Newton's method. When far from the minimum, Newton's method is unstable and can give wildly wrong results, so LM takes gradient descent steps. However, near the minimum it switches to the Newton's method which has \"quadratic convergence\" this means that it takes only a few iterations to converge to several decimal places. This can be represented as:\n",
    "\n",
    "$(H + LI)h = g$\n",
    "\n",
    "Where H is the Hessian matrix of second derivatives, L is the damping parameter, I is the identity matrix, h is the step we will take in parameter space, and g is the gradient. We solve this linear system for h to get the next update step. The \"L\" scale parameter goes from L >> 1 which represents gradient descent to L << 1 which is Newton's Method. When L >> 1 the hessian is effectively zero and we get $h = g/L$ which is just gradient descent with $1/L$ as the learning rate. In AstroPhot the damping parameter is treated somewhat differently, but the concept is the same.\n",
    "\n",
    "LM can handle a lot of scenarios and converge to the minimum. Keep in mind, however, that it is seeking a local minimum, so it is best to start off the algorithm as close as possible to the best fit parameters. AstroPhot can automatically initialize, as discussed in other notebooks, but even that needs help sometimes (often in the form of a segmentation map).\n",
    "\n",
    "The main drawback of LM is its memory consumption which goes as $\\mathcal{O}(PN)$ where P is the number of pixels and N is the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "res_lm = ap.fit.LM(MODEL, verbose=1).fit()\n",
    "print(res_lm.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_init = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL_init)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL_init, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that LM has found the $\\chi^2$ minimum, we can do a really neat trick. Since LM needs the hessian matrix, we have access to the hessian matrix at the minimum. This is in fact equal to the negative Fisher information matrix. If we take the matrix inverse of this matrix then we get the covariance matrix for a multivariate gaussian approximation of the $\\chi^2$ surface near the minimum. With the covariance matrix we can create a corner plot just like we would with an MCMC. We will see later that the MCMC methods (at least the ones which converge) produce very similar results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = list(MODEL.build_params_array_names())\n",
    "set, sky = true_params()\n",
    "corner_plot_covariance(\n",
    "    res_lm.covariance_matrix.detach().cpu().numpy(),\n",
    "    MODEL.build_params_array().detach().cpu().numpy(),\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Fit (models)\n",
    "\n",
    "An iterative fitter is identified as `ap.fit.Iter`, this method is generally employed for large models where it is not feasible to hold all the relevant data in memory at once. The iterative fitter will cycle through the models in a `GroupModel` object and fit them one at a time to the image, using the residuals from the previous cycle. This can be a very robust way to deal with some fits, especially if the overlap between models is not too strong. It is however more dependent on good initialization than other methods like the Levenberg-Marquardt. Also, it is possible for the Iter method to get stuck in a local minimum under certain circumstances.\n",
    "\n",
    "Note that while the Iterative fitter needs a `GroupModel` object to iterate over, it is not necessarily true that the sub models are `ComponentModel` objects, they could be `GroupModel` objects as well. In this way it is possible to cycle through and fit \"clusters\" of objects that are nearby, so long as it doesn't consume too much memory.\n",
    "\n",
    "By only fitting one model at a time it is possible to get caught in a local minimum, or to get out of a local minimum that a different fitter was stuck in. For this reason it can be good to mix-and-match the iterative optimizers so they can help each other get unstuck if a fit is very challenging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "res_iter = ap.fit.Iter(MODEL, verbose=1).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_init = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL_init)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL_init, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Fit (parameters)\n",
    "\n",
    "This is an iterative fitter identified as `ap.fit.IterParam` and is generally employed for complicated models where it is not feasible to hold all the relevant data in memory at once. This iterative fitter will cycle through chunks of parameters and fit them one at a time to the image. This can be a very robust way to deal with some fits, especially if the overlap between models is not too strong. This is very similar to the other iterative fitter, however it is necessary for certain fitting circumstances when the problem can't be broken down into individual component models. This occurs, for example, when the models have many shared (constrained) parameters and there is no obvious way to break down sub-groups of models.\n",
    "\n",
    "Note that this is iterating over the parameters, not the models. This allows it to handle parameter covariances even for very large models (if they happen to land in the same chunk). However, for this to work it must evaluate the whole model at each iteration making it somewhat slower than the regular `Iter` fitter, though it can make up for it by fitting larger chunks at a time which makes the whole optimization faster.\n",
    "\n",
    "By only fitting a subset of parameters at a time it is possible to get caught in a local minimum, or to get out of a local minimum that a different fitter was stuck in. For this reason it can be good to mix-and-match the iterative optimizers so they can help each other get unstuck. Since this iterative fitter chooses parameters randomly, it can sometimes get itself unstuck if it gets a lucky combination of parameters. Generally giving it more parameters to work with at a time is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = initialize_model(target, False)\n",
    "# fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "# plt.subplots_adjust(wspace=0.1)\n",
    "# ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "# axarr[0].set_title(\"Model before optimization\")\n",
    "# ap.plots.residual_image(fig, axarr[1], MODEL, normalize_residuals=True)\n",
    "# axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "# res_iterlm = ap.fit.Iter_LM(MODEL, chunks=11, verbose=1).fit()\n",
    "\n",
    "# ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "# axarr[2].set_title(\"Model after optimization\")\n",
    "# ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "# axarr[3].set_title(\"Residuals after optimization\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy Minimize\n",
    "\n",
    "Any AstroPhot model becomes a function `model(x)` where `x` is a 1D tensor of\n",
    "all the current dynamic parameters. This functional format is common for\n",
    "external packages to use. AstroPhot includes a wrapper to access the\n",
    "`scipy.optimize.minimize` minimizer list. AstroPhot will ensure the minimizers\n",
    "respect the valid ranges set for each parameter.\n",
    "\n",
    "Typically, the AstroPhot LM optimizer is faster and more accurate than the Scipy\n",
    "ones. The exact reason is unclear, but the Scipy minimizers are intended for\n",
    "very general use, while the LM optimizer is specifically optimized for gaussian\n",
    "log likelihoods.\n",
    "\n",
    "In the case below, the minimizer thinks it has terminated successfully, although\n",
    "in fact it is quite far from the minimum. Consider this a lesson in trusting the\n",
    "\"success\" message from an optimizer. It turns out to be very challenging to\n",
    "identify if an optimizer is at a minimum, let alone the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "res_scipy = ap.fit.ScipyFit(MODEL, method=\"SLSQP\", verbose=1).fit()\n",
    "print(res_scipy.scipy_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_init = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL_init)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL_init, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "A gradient descent fitter is identified as `ap.fit.Grad` and uses standard first order derivative methods as provided by PyTorch. These gradient descent methods include Adam, SGD, and LBFGS to name a few. The first order gradient is faster to evaluate and uses less memory, however it is considerably slower to converge than Levenberg-Marquardt. The gradient descent method with a small learning rate will reliably converge towards a local minimum, it will just do so slowly. \n",
    "\n",
    "In the example below we let it run for 1000 steps and even still it has not converged. In general you should not use gradient descent to optimize a model. However, in a challenging fitting scenario the small step size of gradient descent can actually be an advantage as it will not take any unedpectedly large steps which could mix up some models, or hop over the $\\chi^2$ minimum into impossible parameter space. Just make sure to finish with LM after using Grad so that it fully converges to a reliable minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "res_grad = ap.fit.Grad(MODEL, verbose=1, max_iter=1000, optim_kwargs={\"lr\": 5e-2}).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_init = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL_init)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL_init, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Adjusted Langevin Algorithm (MALA)\n",
    "\n",
    "This is one of the simplest gradient based samplers, and is very powerful. The standard Metropolis Hastings algorithm will use a gaussian proposal distribution then use the Metropolis Hastings accept/reject stage. MALA uses gradient information to determine a better proposal distribution locally (while maintaining detailed balance) and then uses the Metropolis Hastings accept/reject stage. We have not integrated this algorithm directly into AstroPhot, instead we write it all out below to show the simplicity and power of the method. Expand the cell below if you are interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def mala_sampler(initial_state, log_prob, log_prob_grad, num_samples, epsilon, mass_matrix):\n",
    "    \"\"\"Metropolis Adjusted Langevin Algorithm (MALA) sampler with batch dimension.\n",
    "\n",
    "    Args:\n",
    "    - initial_state (numpy array): Initial states of the chains, shape (num_chains, dim).\n",
    "    - log_prob (function): Function to compute the log probabilities of the current states.\n",
    "    - log_prob_grad (function): Function to compute the gradients of the log probabilities.\n",
    "    - num_samples (int): Number of samples to generate.\n",
    "    - epsilon (float): Step size for the Langevin dynamics.\n",
    "    - mass_matrix (numpy array): Mass matrix, shape (dim, dim), used to scale the dynamics.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - samples (numpy array): Array of sampled values, shape (num_samples, num_chains, dim).\n",
    "    \"\"\"\n",
    "    num_chains, dim = initial_state.shape\n",
    "    samples = np.zeros((num_samples, num_chains, dim))\n",
    "    x_current = np.array(initial_state)\n",
    "    current_log_prob = log_prob(x_current)\n",
    "    inv_mass_matrix = np.linalg.inv(mass_matrix)\n",
    "    chol_inv_mass_matrix = np.linalg.cholesky(inv_mass_matrix)\n",
    "\n",
    "    pbar = tqdm(range(num_samples))\n",
    "    acceptance_rate = np.zeros([0])\n",
    "    for i in pbar:\n",
    "        gradients = log_prob_grad(x_current)\n",
    "        noise = np.dot(np.random.randn(num_chains, dim), chol_inv_mass_matrix.T)\n",
    "        proposal = (\n",
    "            x_current + 0.5 * epsilon**2 * np.dot(gradients, inv_mass_matrix) + epsilon * noise\n",
    "        )\n",
    "\n",
    "        # proposal = x_current + 0.5 * epsilon**2 * gradients + epsilon * np.random.randn(num_chains, *dim)\n",
    "        proposal_log_prob = log_prob(proposal)\n",
    "        # Metropolis-Hastings acceptance criterion, computed for each chain\n",
    "        acceptance_log_prob = proposal_log_prob - current_log_prob\n",
    "        accept = np.log(np.random.rand(num_chains)) < acceptance_log_prob\n",
    "        acceptance_rate = np.concatenate([acceptance_rate, accept])\n",
    "        pbar.set_description(f\"Acceptance rate: {acceptance_rate.mean():.2f}\")\n",
    "\n",
    "        # Update states where accepted\n",
    "        x_current[accept] = proposal[accept]\n",
    "        current_log_prob[accept] = proposal_log_prob[accept]\n",
    "\n",
    "        samples[i] = x_current\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "# Use LM to start the sampler at a high likelihood location, no burn-in needed!\n",
    "res1 = ap.fit.LM(MODEL).fit()\n",
    "\n",
    "\n",
    "def density(x):\n",
    "    x = torch.as_tensor(x, dtype=ap.config.DTYPE)\n",
    "    return torch.vmap(MODEL.gaussian_log_likelihood)(x).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "sim_grad = torch.vmap(torch.func.grad(MODEL.gaussian_log_likelihood))\n",
    "\n",
    "\n",
    "def density_grad(x):\n",
    "    x = torch.as_tensor(x, dtype=ap.config.DTYPE)\n",
    "    return sim_grad(x).numpy()\n",
    "\n",
    "\n",
    "x0 = MODEL.build_params_array().detach().cpu().numpy()\n",
    "x0 = x0 + np.random.normal(scale=0.001, size=(8, x0.shape[0]))\n",
    "chain_mala = mala_sampler(\n",
    "    initial_state=x0,\n",
    "    log_prob=density,\n",
    "    log_prob_grad=density_grad,\n",
    "    num_samples=300,\n",
    "    epsilon=2e-1,\n",
    "    mass_matrix=torch.linalg.inv(res1.covariance_matrix).detach().cpu().numpy(),\n",
    ")\n",
    "chain_mala = chain_mala.reshape(-1, chain_mala.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# # corner plot of the posterior\n",
    "param_names = list(MODEL.build_params_array_names())\n",
    "\n",
    "set, sky = true_params()\n",
    "corner_plot(\n",
    "    chain_mala,\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte-Carlo (HMC)\n",
    "\n",
    "The `ap.fit.HMC` takes a fixed number of steps at a fixed step size following Hamiltonian dynamics. This is in contrast to NUTS which attempts to optimally choose these parameters. The simplest way to think of HMC is as performing a number of MALA steps all in one go, so if `leapfrog_steps = 10` then HMC is very similar to running MALA then taking every tenth step and adding it to the chain. HMC results will still have autocorrelation which will depend on the problem and choice of step parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "# Use LM to start the sampler at a high likelihood location, no burn-in needed!\n",
    "res1 = ap.fit.LM(MODEL).fit()\n",
    "\n",
    "# Run the HMC sampler\n",
    "res_hmc = ap.fit.HMC(\n",
    "    MODEL,\n",
    "    warmup=1,\n",
    "    max_iter=150,\n",
    "    epsilon=1e-1,\n",
    "    leapfrog_steps=10,\n",
    "    inv_mass=res1.covariance_matrix,\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# corner plot of the posterior\n",
    "param_names = list(MODEL.build_params_array_names())\n",
    "\n",
    "set, sky = true_params()\n",
    "corner_plot(\n",
    "    res_hmc.chain.detach().cpu().numpy(),\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings\n",
    "\n",
    "This is the more standard MCMC algorithm using the Metropolis Hastngs accept step identified with `ap.fit.MHMCMC`. Under the hood, this is just a wrapper for the excellent `emcee` package, if you want to take advantage of more `emcee` features you can very easily use `ap.fit.MHMCMC` as a starting point. However, one should keep in mind that for large models it can take exceedingly long to actually converge to the posterior. Instead of waiting that long, we demonstrate the functionality with 100 steps (and 30 chains), but suggest using MALA for any real world problem. Still, if there is something NUTS can't handle (a function that isn't differentiable) then MHMCMC can save the day (even if it takes all day to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "# Use LM to start the sampler at a high likelihood location, no burn-in needed!\n",
    "print(\"running LM fit\")\n",
    "res1 = ap.fit.LM(MODEL).fit()\n",
    "\n",
    "# Run the HMC sampler\n",
    "print(\"running MHMCMC sampling\")\n",
    "res_mh = ap.fit.MHMCMC(MODEL, verbose=1, max_iter=100).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corner plot of the posterior\n",
    "# note that, even 3000 samples is not enough to overcome the autocorrelation so the posterior has not converged.\n",
    "param_names = list(MODEL.build_params_array_names())\n",
    "\n",
    "set, sky = true_params()\n",
    "corner_plot(\n",
    "    res_mh.chain[::10],  # thin by a factor 10 so the plot works in reasonable time\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
