{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Methods\n",
    "\n",
    "Here we will explore the various fitting methods in AstroPhot. You have already encountered some of the methods, but here we will take a more systematic approach and discuss their strengths/weaknesses. Each method will be applied to the same problem with the same initial conditions so you can see how they operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import gaussian_kde as kde\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib inline\n",
    "import astrophot as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a fitting problem. You can ignore this cell to start, it just makes some test data to fit\n",
    "\n",
    "\n",
    "def true_params():\n",
    "\n",
    "    # just some random parameters to use for fitting. Feel free to play around with these to see what happens!\n",
    "    sky_param = np.array([1.5])\n",
    "    sersic_params = np.array(\n",
    "        [\n",
    "            [\n",
    "                58.44035491,\n",
    "                55.58516735,\n",
    "                0.54945988,\n",
    "                37.19794926 * np.pi / 180,\n",
    "                2.14513004,\n",
    "                22.05219055,\n",
    "                2.45583024,\n",
    "            ],\n",
    "            [\n",
    "                44.00353786,\n",
    "                31.54430634,\n",
    "                0.40203928,\n",
    "                172.03862521 * np.pi / 180,\n",
    "                2.88613347,\n",
    "                12.095631,\n",
    "                2.76711163,\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return sersic_params, sky_param\n",
    "\n",
    "\n",
    "def init_params():\n",
    "\n",
    "    sky_param = np.array([1.4])\n",
    "    sersic_params = np.array(\n",
    "        [\n",
    "            [57.0, 56.0, 0.6, 40.0 * np.pi / 180, 1.5, 25.0, 2.0],\n",
    "            [45.0, 30.0, 0.5, 170.0 * np.pi / 180, 2.0, 10.0, 3.0],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return sersic_params, sky_param\n",
    "\n",
    "\n",
    "def initialize_model(target, use_true_params=True):\n",
    "\n",
    "    # Pick parameters to start the model with\n",
    "    if use_true_params:\n",
    "        sersic_params, sky_param = true_params()\n",
    "    else:\n",
    "        sersic_params, sky_param = init_params()\n",
    "\n",
    "    # List of models, starting with the sky\n",
    "    model_list = [\n",
    "        ap.models.AstroPhot_Model(\n",
    "            name=\"sky\",\n",
    "            model_type=\"flat sky model\",\n",
    "            target=target,\n",
    "            parameters={\"F\": sky_param[0]},\n",
    "        )\n",
    "    ]\n",
    "    # Add models to the list\n",
    "    for i, params in enumerate(sersic_params):\n",
    "        model_list.append(\n",
    "            [\n",
    "                ap.models.AstroPhot_Model(\n",
    "                    name=f\"sersic {i}\",\n",
    "                    model_type=\"sersic galaxy model\",\n",
    "                    target=target,\n",
    "                    parameters={\n",
    "                        \"center\": [params[0], params[1]],\n",
    "                        \"q\": params[2],\n",
    "                        \"PA\": params[3],\n",
    "                        \"n\": params[4],\n",
    "                        \"Re\": params[5],\n",
    "                        \"Ie\": params[6],\n",
    "                    },\n",
    "                    # psf_mode = \"full\", # uncomment to try everything with PSF blurring (takes longer)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    MODEL = ap.models.Group_Model(\n",
    "        name=\"group\",\n",
    "        models=model_list,\n",
    "        target=target,\n",
    "    )\n",
    "    # Make sure every model is ready to go\n",
    "    MODEL.initialize()\n",
    "\n",
    "    return MODEL\n",
    "\n",
    "\n",
    "def generate_target():\n",
    "\n",
    "    N = 99\n",
    "    pixelscale = 1.0\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # PSF has sigma of 2x pixelscale\n",
    "    PSF = ap.utils.initialize.gaussian_psf(2, 21, pixelscale)\n",
    "    PSF /= np.sum(PSF)\n",
    "\n",
    "    target = ap.image.Target_Image(\n",
    "        data=np.zeros((N, N)),\n",
    "        pixelscale=pixelscale,\n",
    "        psf=PSF,\n",
    "    )\n",
    "\n",
    "    MODEL = initialize_model(target, True)\n",
    "\n",
    "    # Sample the model with the true values to make a mock image\n",
    "    img = MODEL().data.detach().cpu().numpy()\n",
    "    # Add poisson noise\n",
    "    target.data = torch.Tensor(img + rng.normal(scale=np.sqrt(img) / 2))\n",
    "    target.variance = torch.Tensor(img / 4)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ap.plots.target_image(fig, ax, target)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def corner_plot(\n",
    "    chain,\n",
    "    labels=None,\n",
    "    bins=None,\n",
    "    true_values=None,\n",
    "    plot_density=True,\n",
    "    plot_contours=True,\n",
    "    figsize=(10, 10),\n",
    "):\n",
    "    ndim = chain.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim, ndim, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "    if bins is None:\n",
    "        bins = int(np.sqrt(chain.shape[0]))\n",
    "\n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            i_range = (np.min(chain[:, i]), np.max(chain[:, i]))\n",
    "            j_range = (np.min(chain[:, j]), np.max(chain[:, j]))\n",
    "            if i == j:\n",
    "                # Plot the histogram of parameter i\n",
    "                # ax.hist(chain[:, i], bins=bins, histtype=\"step\", range = i_range, density=True, color=\"k\", lw=1)\n",
    "\n",
    "                if plot_density:\n",
    "                    # Plot the kernel density estimate\n",
    "                    kde_x = np.linspace(i_range[0], i_range[1], 100)\n",
    "                    kde_y = kde(chain[:, i])(kde_x)\n",
    "                    ax.plot(kde_x, kde_y, color=\"green\", lw=1)\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                ax.set_xlim(i_range)\n",
    "\n",
    "            elif i > j:\n",
    "                # Plot the 2D histogram of parameters i and j\n",
    "                # ax.hist2d(chain[:, j], chain[:, i], bins=bins, cmap=\"Greys\")\n",
    "\n",
    "                if plot_contours:\n",
    "                    # Plot the kernel density estimate contours\n",
    "                    kde_ij = kde([chain[:, j], chain[:, i]])\n",
    "                    x, y = np.mgrid[j_range[0] : j_range[1] : 100j, i_range[0] : i_range[1] : 100j]\n",
    "                    positions = np.vstack([x.ravel(), y.ravel()])\n",
    "                    kde_pos = np.reshape(kde_ij(positions).T, x.shape)\n",
    "                    ax.contour(x, y, kde_pos, colors=\"green\", linewidths=1, levels=3)\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[j], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                    ax.axhline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                ax.set_xlim(j_range)\n",
    "                ax.set_ylim(i_range)\n",
    "\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            if j == 0 and labels is not None:\n",
    "                ax.set_ylabel(labels[i])\n",
    "            ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "            if i == ndim - 1 and labels is not None:\n",
    "                ax.set_xlabel(labels[j])\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def corner_plot_covariance(\n",
    "    cov_matrix, mean, labels=None, figsize=(10, 10), true_values=None, ellipse_colors=\"g\"\n",
    "):\n",
    "    num_params = cov_matrix.shape[0]\n",
    "    fig, axes = plt.subplots(num_params, num_params, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "\n",
    "    for i in range(num_params):\n",
    "        for j in range(num_params):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                x = np.linspace(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    100,\n",
    "                )\n",
    "                y = norm.pdf(x, mean[i], np.sqrt(cov_matrix[i, i]))\n",
    "                ax.plot(x, y, color=\"g\")\n",
    "                ax.set_xlim(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]), mean[i] + 3 * np.sqrt(cov_matrix[i, i])\n",
    "                )\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "            elif j < i:\n",
    "                cov = cov_matrix[np.ix_([j, i], [j, i])]\n",
    "                lambda_, v = np.linalg.eig(cov)\n",
    "                lambda_ = np.sqrt(lambda_)\n",
    "                angle = np.rad2deg(np.arctan2(v[1, 0], v[0, 0]))\n",
    "                for k in [1, 2]:\n",
    "                    ellipse = Ellipse(\n",
    "                        xy=(mean[j], mean[i]),\n",
    "                        width=lambda_[0] * k * 2,\n",
    "                        height=lambda_[1] * k * 2,\n",
    "                        angle=angle,\n",
    "                        edgecolor=ellipse_colors,\n",
    "                        facecolor=\"none\",\n",
    "                    )\n",
    "                    ax.add_artist(ellipse)\n",
    "\n",
    "                # Set axis limits\n",
    "                margin = 3\n",
    "                ax.set_xlim(\n",
    "                    mean[j] - margin * np.sqrt(cov_matrix[j, j]),\n",
    "                    mean[j] + margin * np.sqrt(cov_matrix[j, j]),\n",
    "                )\n",
    "                ax.set_ylim(\n",
    "                    mean[i] - margin * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + margin * np.sqrt(cov_matrix[i, i]),\n",
    "                )\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[j], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                    ax.axhline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "\n",
    "            if j > i:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            if i < num_params - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_xlabel(labels[j])\n",
    "            ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "target = generate_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt\n",
    "\n",
    "This fitter is identitied as `ap.fit.LM` and it employs a variant of the second order Newton's method to converge very quickly to the local minimum. This is the generally accepted best algorithm for most use cases in $\\chi^2$ minimization. If you don't know what to pick, start with this minimizer. The LM optimizer bridges the gap between first-order gradient descent and second order Newton's method. When far from the minimum, Newton's method is unstable and can give wildly wrong results, so LM takes gradient descent steps. However, near the minimum it switches to the Newton's method which has \"quadratic convergence\" this means that it takes only a few iterations to converge to several decimal places. This can be represented as:\n",
    "\n",
    "$(H + LI)h = g$\n",
    "\n",
    "Where H is the Hessian matrix of second derivatives, L is the damping parameter, I is the identity matrix, h is the step we will take in parameter space, and g is the gradient. We solve this linear system for h to get the next update step. The \"L\" scale parameter goes from L >> 1 which represents gradient descent to L << 1 which is Newton's Method. When L >> 1 the hessian is effectively zero and we get $h = g/L$ which is just gradient descent with $1/L$ as the learning rate. In AstroPhot the damping parameter is treated somewhat differently, but the concept is the same.\n",
    "\n",
    "LM can handle a lot of scenarios and converge to the minimum. Keep in mind, however, that it is seeking a local minimum, so it is best to start off the algorithm as close as possible to the best fit parameters. AstroPhot can automatically initialize, as discussed in other notebooks, but even that needs help sometimes (often in the form of a segmentation map).\n",
    "\n",
    "The main drawback of LM is its memory consumption which goes as $\\mathcal{O}(PN)$ where P is the number of pixels and N is the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res_lm = ap.fit.LM(MODEL, verbose=1).fit()\n",
    "print(res_lm.message)\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that LM has found the $\\chi^2$ minimum, we can do a really neat trick. Since LM needs the hessian matrix, we have access to the hessian matrix at the minimum. This is in fact equal to the negative Fisher information matrix. If we take the matrix inverse of this matrix then we get the covariance matrix for a multivariate gaussian approximation of the $\\chi^2$ surface near the minimum. With the covariance matrix we can create a corner plot just like we would with an MCMC. We will see later that the MCMC methods (at least the ones which converge) produce very similar results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = list(MODEL.parameters.vector_names())\n",
    "i = 0\n",
    "while i < len(param_names):\n",
    "    param_names[i] = param_names[i].replace(\" \", \"\")\n",
    "    if \"center\" in param_names[i]:\n",
    "        center_name = param_names.pop(i)\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"y\"))\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"x\"))\n",
    "    i += 1\n",
    "set, sky = true_params()\n",
    "corner_plot_covariance(\n",
    "    res_lm.covariance_matrix.detach().cpu().numpy(),\n",
    "    MODEL.parameters.vector_values().detach().cpu().numpy(),\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Fit (models)\n",
    "\n",
    "An iterative fitter is identified as `ap.fit.Iter`, this method is generally employed for large models where it is not feasible to hold all the relevant data in memory at once. The iterative fitter will cycle through the models in a `Group_Model` object and fit them one at a time to the image, using the residuals from the previous cycle. This can be a very robust way to deal with some fits, especially if the overlap between models is not too strong. It is however more dependent on good initialization than other methods like the Levenberg-Marquardt. Also, it is possible for the Iter method to get stuck in a local minimum under certain circumstances.\n",
    "\n",
    "Note that while the Iterative fitter needs a `Group_Model` object to iterate over, it is not necessarily true that the sub models are `Component_Model` objects, they could be `Group_Model` objects as well. In this way it is possible to cycle through and fit \"clusters\" of objects that are nearby, so long as it doesn't consume too much memory.\n",
    "\n",
    "By only fitting one model at a time it is possible to get caught in a local minimum, or to get out of a local minimum that a different fitter was stuck in. For this reason it can be good to mix-and-match the iterative optimizers so they can help each other get unstuck if a fit is very challenging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res_iter = ap.fit.Iter(MODEL, verbose=1).fit()\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Fit (parameters)\n",
    "\n",
    "This is an iterative fitter identified as `ap.fit.Iter_LM` and is generally employed for large models where it is not feasible to hold all the relevant data in memory at once. This iterative fitter will cycle through chunks of parameters and fit them one at a time to the image. This can be a very robust way to deal with some fits, especially if the overlap between models is not too strong. This is very similar to the other iterative fitter, however it is necessary for certain fitting circumstances when the problem can't be broken down into individual component models. This occurs, for example, when the models have many shared (constrained) parameters and there is no obvious way to break down sub-groups of models (an example of this is discussed in the AstroPhot paper).\n",
    "\n",
    "Note that this is iterating over the parameters, not the models. This allows it to handle parameter covariances even for very large models (if they happen to land in the same chunk). However, for this to work it must evaluate the whole model at each iteration making it somewhat slower than the regular `Iter` fitter, though it can make up for it by fitting larger chunks at a time which makes the whole optimization faster.\n",
    "\n",
    "By only fitting a subset of parameters at a time it is possible to get caught in a local minimum, or to get out of a local minimum that a different fitter was stuck in. For this reason it can be good to mix-and-match the iterative optimizers so they can help each other get unstuck. Since this iterative fitter chooses parameters randomly, it can sometimes get itself unstuck if it gets a lucky combination of parameters. Generally giving it more parameters to work with at a time is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res_iterlm = ap.fit.Iter_LM(MODEL, chunks=11, verbose=1).fit()\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "A gradient descent fitter is identified as `ap.fit.Grad` and uses standard first order derivative methods as provided by PyTorch. These gradient descent methods include Adam, SGD, and LBFGS to name a few. The first order gradient is faster to evaluate and uses less memory, however it is considerably slower to converge than Levenberg-Marquardt. The gradient descent method with a small learning rate will reliably converge towards a local minimum, it will just do so slowly. \n",
    "\n",
    "In the example below we let it run for 1000 steps and even still it has not converged. In general you should not use gradient descent to optimize a model. However, in a challenging fitting scenario the small step size of gradient descent can actually be an advantage as it will not take any unedpectedly large steps which could mix up some models, or hop over the $\\chi^2$ minimum into impossible parameter space. Just make sure to finish with LM after using Grad so that it fully converges to a reliable minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1, 4, figsize=(24, 5))\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL, normalize_residuals=True)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res_grad = ap.fit.Grad(MODEL, verbose=1, max_iter=1000, optim_kwargs={\"lr\": 5e-3}).fit()\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL, normalize_residuals=True)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No U-Turn Sampler (NUTS)\n",
    "\n",
    "Unlike the above methods, `ap.fit.NUTS` does not stricktly seek a minimum $\\chi^2$, instead it is an MCMC method which seeks to explore the likelihood space and provide a full posterior in the form of random samples. The NUTS method in AstroPhot is actually just a wrapper for the Pyro implementation (__[link here](https://docs.pyro.ai/en/stable/index.html)__). Most of the functionality can be accessed this way, though for very advanced applications it may be necessary to manually interface with Pyro (this is not very challenging as AstroPhot is fully differentiable).\n",
    "\n",
    "The first iteration of NUTS is always very slow since it compiles the forward method on the fly, after that each sample is drawn much faster. The warmup iterations take longer as the method is exploring the space and determining the ideal step size and mass matrix for fast integration with minimal numerical error (we only do 20 warmup steps here, if something goes wrong just try rerunning). Once the algorithm begins sampling it is able to move quickly (for an MCMC) through the parameter space. For many models, the NUTS sampler is able to collect nearly completely uncorrelated samples, meaning that even 100 is enough to get a good estimate of the posterior.\n",
    "\n",
    "NUTS is far faster than other MCMC implementations such as a standard Metropolis Hastings MCMC. However, it is still a lot slower than the other optimizers (LM) since it is doing more than seeking a single high likelihood point, it is fully exploring the likelihood space. In simple cases, the automatic covariance matrix from LM is likely good enough, but if one really needs access to the full posterior of a complex model then NUTS is the best way to get it.\n",
    "\n",
    "For an excellent introduction to the Hamiltonian Monte-Carlo and a high level explanation of NUTS see this review:\n",
    "__[Betancourt 2018](https://arxiv.org/pdf/1701.02434.pdf)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "# Use LM to start the sampler at a high likelihood location, no burn-in needed!\n",
    "# In general, NUTS is quite fast to do burn-in so this is often not needed\n",
    "res1 = ap.fit.LM(MODEL).fit()\n",
    "\n",
    "# Run the NUTS sampler\n",
    "res_nuts = ap.fit.NUTS(\n",
    "    MODEL,\n",
    "    warmup=20,\n",
    "    max_iter=100,\n",
    "    inv_mass=res1.covariance_matrix,\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no \"after optimization\" image above, because optimization was not done, it was full likelihood exploration. We can now create a corner plot with 2D projections of the 22 dimensional space that NUTS was exploring. The resulting corner plot is about what you would expect to get with 100 samples drawn from the multivariate gaussian found by LM above. If you run it again with more samples then the results will get even smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corner plot of the posterior\n",
    "# observe that it is very similar to the corner plot from the LM optimization since this case can be roughly\n",
    "# approximated as a multivariate gaussian centered on the maximum likelihood point\n",
    "param_names = list(MODEL.parameters.vector_names())\n",
    "i = 0\n",
    "while i < len(param_names):\n",
    "    param_names[i] = param_names[i].replace(\" \", \"\")\n",
    "    if \"center\" in param_names[i]:\n",
    "        center_name = param_names.pop(i)\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"y\"))\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"x\"))\n",
    "    i += 1\n",
    "\n",
    "set, sky = true_params()\n",
    "corner_plot(\n",
    "    res_nuts.chain.detach().cpu().numpy(),\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte-Carlo (HMC)\n",
    "\n",
    "The `ap.fit.HMC` is a simpler variant of the NUTS sampler. HMC takes a fixed number of steps at a fixed step size following Hamiltonian dynamics. This is in contrast to NUTS which attempts to optimally choose these parameters. HMC may be suitable in some cases where NUTS is unable to find ideal parameters. Also in some cases where you already know the pretty good step parameters HMC may run faster. If you don't want to fiddle around with parameters then stick with NUTS, HMC results will still have autocorrelation which will depend on the problem and choice of step parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "# Use LM to start the sampler at a high likelihood location, no burn-in needed!\n",
    "res1 = ap.fit.LM(MODEL).fit()\n",
    "\n",
    "# Run the HMC sampler\n",
    "res_hmc = ap.fit.HMC(\n",
    "    MODEL,\n",
    "    warmup=1,\n",
    "    max_iter=150,\n",
    "    epsilon=1e-1,\n",
    "    leapfrog_steps=10,\n",
    "    inv_mass=res1.covariance_matrix,\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corner plot of the posterior\n",
    "param_names = list(MODEL.parameters.vector_names())\n",
    "i = 0\n",
    "while i < len(param_names):\n",
    "    param_names[i] = param_names[i].replace(\" \", \"\")\n",
    "    if \"center\" in param_names[i]:\n",
    "        center_name = param_names.pop(i)\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"y\"))\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"x\"))\n",
    "    i += 1\n",
    "\n",
    "set, sky = true_params()\n",
    "corner_plot(\n",
    "    res_hmc.chain.detach().cpu().numpy(),\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings\n",
    "\n",
    "This is the classic MCMC algorithm using the Metropolis Hastngs accept step identified with `ap.fit.MHMCMC`. One can set the gaussian random step scale and then explore the posterior. While this technically always works, in practice it can take exceedingly long to actually converge to the posterior. This is because the step size must be set very small to have a reasonable likelihood of accepting each step, so it never moves very far in parameter space. With each subsequent sample being very close to the previous sample it can take a long time for it to wander away from its starting point. In the example below it would take an extremely long time for the chain to converge. Instead of waiting that long, we demonstrate the functionality with 1000 steps, but suggest using NUTS for any real world problem. Still, if there is something NUTS can't handle (a function that isn't differentiable) then MHMCMC can save the day (even if it takes all day to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "\n",
    "# Use LM to start the sampler at a high likelihood location, no burn-in needed!\n",
    "res1 = ap.fit.LM(MODEL).fit()\n",
    "\n",
    "# Run the HMC sampler\n",
    "res_mh = ap.fit.MHMCMC(MODEL, verbose=1, max_iter=1000, epsilon=1e-4, report_after=np.inf).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corner plot of the posterior\n",
    "# note that, even 1000 samples is not enough to overcome the autocorrelation so the posterior has not converged.\n",
    "# In fact it is not even close to convergence as can be seen by the multi-modal blobs in the posterior since this\n",
    "# problem is unimodal (except the modes where models are swapped). It is almost never worthwhile to use this\n",
    "# sampler except as a sanity check on very simple models.\n",
    "param_names = list(MODEL.parameters.vector_names())\n",
    "i = 0\n",
    "while i < len(param_names):\n",
    "    param_names[i] = param_names[i].replace(\" \", \"\")\n",
    "    if \"center\" in param_names[i]:\n",
    "        center_name = param_names.pop(i)\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"y\"))\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"x\"))\n",
    "    i += 1\n",
    "\n",
    "set, sky = true_params()\n",
    "corner_plot(\n",
    "    res_mh.chain[::10],  # thin by a factor 10 so the plot works in reasonable time\n",
    "    labels=param_names,\n",
    "    figsize=(20, 20),\n",
    "    true_values=np.concatenate((sky, set.ravel())),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
