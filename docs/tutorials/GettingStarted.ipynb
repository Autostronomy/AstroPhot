{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with AutoProf\n",
    "\n",
    "In this notebook you will walk through the very basics of AutoProf functionality. Here you will learn how to make models; how to set them up for fitting; and how to view the results. These core elements will come up every time you use AutoProf, though in future notebooks you will learn how to take advantage of the advanced features in AutoProf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autoprof as ap\n",
    "import numpy as np\n",
    "import torch\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first model\n",
    "\n",
    "The basic format for making an AutoProf model is given below. Once a model object is constructed, it can be manipulated and updated in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ap.models.AutoProf_Model(\n",
    "    name = \"model1\", # every model must have a unique name\n",
    "    model_type = \"sersic galaxy model\", # this specifies the kind of model\n",
    "    parameters = {\"center\": [50,50], \"q\": 0.6, \"PA\": 60*np.pi/180, \"n\": 2, \"Re\": 10, \"Ie\": 1}, # here we set initial values for each parameter\n",
    "    target = ap.image.Target_Image(np.zeros((100,100)), pixelscale = 1), # every model needs a target, more on this later\n",
    ")\n",
    "model1.initialize() # before using the model it is good practice to call initialize so the model can get itself ready\n",
    "\n",
    "# We can print the model's basic info\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoProf has built in methods to plot relevant information. We didn't specify the region on the sky for\n",
    "# this model to focus on, so we just made a 100x100 window. Unless you are very lucky this wont\n",
    "# line up with what you're trying to fit, so next we'll see how to give the model a target.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "ap.plots.model_image(fig, ax, model1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving the model a Target\n",
    "\n",
    "Typically, the main goal when constructing an AutoProf model is to fit to an image. We need to give the model access to the image and some information about it to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's download an image to play with\n",
    "hdu = fits.open(\"https://www.legacysurvey.org/viewer/fits-cutout?ra=36.3684&dec=-25.6389&size=700&layer=ls-dr9&pixscale=0.262&bands=r\")\n",
    "target_data = np.array(hdu[0].data, dtype = np.float64)\n",
    "\n",
    "# Create a target object with specified pixelscale and zeropoint\n",
    "target = ap.image.Target_Image(\n",
    "    data = target_data,\n",
    "    pixelscale = 0.262, # Every target image needs to know it's pixelscale in arcsec/pixel\n",
    "    zeropoint = 22.5, # optionally, you can give a zeropoint to tell AutoProf what the pixel flux units are\n",
    ")\n",
    "\n",
    "# The default AutoProf target plotting method uses log scaling in bright areas and histogram scaling in faint areas\n",
    "fig3, ax3 = plt.subplots(figsize = (8,8))\n",
    "ap.plots.target_image(fig3, ax3, target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model now has a target that it will attempt to match\n",
    "model2 = ap.models.AutoProf_Model(\n",
    "    name = \"model with target\", \n",
    "    model_type = \"sersic galaxy model\", # feel free to swap out sersic with other profile types\n",
    "    target = target, # now the model knows what its trying to match\n",
    ")\n",
    "\n",
    "# Instead of giving initial values for all the parameters, it is possible to simply call \"initialize\" and AutoProf \n",
    "# will try to guess initial values for every parameter assuming the galaxy is roughly centered. It is also possible\n",
    "# to set just a few parameters and let AutoProf try to figure out the rest. For example you could give it an initial\n",
    "# Guess for the center and it will work from there.\n",
    "model2.initialize()\n",
    "\n",
    "# Plotting the initial parameters and residuals, we see it gets the rough shape of the galaxy right, but still has some fitting to do\n",
    "fig4, ax4 = plt.subplots(1, 2, figsize = (16,7))\n",
    "ap.plots.model_image(fig4, ax4[0], model2)\n",
    "ap.plots.residual_image(fig4, ax4[1], model2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model has been set up with a target and initialized with parameter values, it is time to fit the image\n",
    "result = ap.fit.LM(model2, verbose = 1).fit()\n",
    "\n",
    "# See that we use ap.fit.LM, this is the Levenberg-Marquardt Chi^2 minimization method, it is the recommended technique\n",
    "# for most least-squares problems. However, there are situations in which different optimizers may be more desireable\n",
    "# so the ap.fit package includes a few options to pick from. The various fitting methods will be described in a \n",
    "# different tutorial.\n",
    "print(\"Fit message:\",result.message) # the fitter will return a message about its convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now plot the fitted model and the image residuals\n",
    "fig5, ax5 = plt.subplots(1, 2, figsize = (16,7))\n",
    "ap.plots.model_image(fig5, ax5[0], model2)\n",
    "ap.plots.residual_image(fig5, ax5[1], model2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot surface brightness profile\n",
    "\n",
    "# we now plot the model profile and a data profile. The model profile is determined from the model parameters\n",
    "# the data profile is determined by taking the median of pixel values at a given radius. Notice that the model\n",
    "# profile is slightly higher than the data profile? This is because there are other objects in the image which\n",
    "# are not being modelled, the data profile uses a median so they are ignored, but for the model we fit all pixels.\n",
    "fig10, ax10 = plt.subplots(figsize = (8,8))\n",
    "ap.plots.galaxy_light_profile(fig10, ax10, model2)\n",
    "ap.plots.radial_median_profile(fig10, ax10, model2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving the model a specific target window\n",
    "\n",
    "Sometimes an object isn't nicely centered in the image, and may not even be the dominant object in the image. It is therefore nice to be able to specify what part of the image we should analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ap.models.AutoProf_Model(\n",
    "    name = \"model with target\", \n",
    "    model_type = \"sersic galaxy model\",\n",
    "    target = target,\n",
    "    window = [[480, 590],[555, 665]], # this is a region in pixel coordinates ((xmin,xmax),(ymin,ymax)) \n",
    ")\n",
    "\n",
    "# We can plot the \"model window\" to show us what part of the image will be analyzed by that model\n",
    "fig6, ax6 = plt.subplots(figsize = (8,8))\n",
    "ap.plots.target_image(fig6, ax6, model3.target)\n",
    "ap.plots.model_window(fig6, ax6, model3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.initialize()\n",
    "result = ap.fit.LM(model3, verbose = 1).fit()\n",
    "print(result.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that when only a window is fit, the default plotting methods will only show that window\n",
    "fig7, ax7 = plt.subplots(1, 2, figsize = (16,7))\n",
    "ap.plots.model_image(fig7, ax7[0], model3)\n",
    "ap.plots.residual_image(fig7, ax7[1], model3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameter constraints\n",
    "\n",
    "A common feature of fitting parameters is that they have some constraint on their behaviour and cannot be sampled at any value from (-inf, inf). AutoProf circumvents this by remapping any constrained parameter to a space where it can take any real value, at least for the sake of fitting. For most parameters these constraints are applied by default; for example the axis ratio q is required to be in the range (0,1). Other parameters, such as the position angle (PA) are cyclic, they can be in the range (0,pi) but also can wrap around. It is possible to manually set these constraints while constructing a model.\n",
    "\n",
    "In general adding constraints makes fitting more difficult. There is a chance that the fitting process runs up against a constraint boundary and gets stuck. However, sometimes adding constraints is necessary and so the capability is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we make a sersic model that can only have q and n in a narrow range\n",
    "# Also, we give PA and initial value and lock that so it does not change during fitting\n",
    "constrained_param_model = ap.models.AutoProf_Model(\n",
    "    name = \"constrained parameters\", model_type = \"sersic galaxy model\", \n",
    "    parameters = {\n",
    "        \"q\": {\"limits\": [0.4,0.6]}, \n",
    "        \"n\": {\"limits\": [2,3]}, \n",
    "        \"PA\": {\"value\": 60*np.pi/180, \"locked\": True},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from constraints on an individual parameter, it is sometimes desireable to have different models share parameter values. For example you may wish to combine multiple simple models into a more complex model (more on that in a different tutorial), and you may wish for them all to have the same center. This can be accomplished with \"equality constraints\" as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 is a sersic model\n",
    "model_1 = ap.models.AutoProf_Model(\n",
    "    name = \"constrained 1\",\n",
    "    model_type = \"sersic galaxy model\",\n",
    "    parameters = {\"center\": [50,50], \"PA\": np.pi/4}\n",
    ")\n",
    "# model 2 is an exponential model\n",
    "model_2 = ap.models.AutoProf_Model(\n",
    "    name = \"constrained 2\",\n",
    "    model_type = \"exponential galaxy model\",\n",
    ")\n",
    "\n",
    "# Here we add the constraint for \"PA\" to be the same for each model.\n",
    "# In doing so we provide the model and parameter name which should \n",
    "# be connected.\n",
    "model_2.add_equality_constraint(model_1, \"PA\")\n",
    "\n",
    "# Here we can see how the two models now both can modify this parameter\n",
    "print(\"initial values: model_1 PA\", model_1[\"PA\"].value.item(), \"model_2 PA\", model_2[\"PA\"].value.item())\n",
    "# Now we modify the PA for model_1\n",
    "model_1[\"PA\"].value = np.pi/3\n",
    "print(\"change model_1: model_1 PA\", model_1[\"PA\"].value.item(), \"model_2 PA\", model_2[\"PA\"].value.item())\n",
    "# Similarly we modify the PA for model_2\n",
    "model_2[\"PA\"].value = np.pi/2\n",
    "print(\"change model_2: model_1 PA\", model_1[\"PA\"].value.item(), \"model_2 PA\", model_2[\"PA\"].value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind that both models have full control over the parameter, it is listed in both of\n",
    "# their \"parameter_order\" tuples. The built-in AutoProf functions keep track of constrained \n",
    "# parameters by asking models if any of their parameters are constrained\n",
    "print(\"model_1 parameters: \", model_1.parameter_order(), \" are any parameter constrained: \", model_1.equality_constraints)\n",
    "print(\"model_2 parameters: \", model_2.parameter_order(), \" are any parameter constrained: \", model_2.equality_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSF convolution\n",
    "\n",
    "An important part of astronomical image analysis is accounting for PSF effects. To that end, AutoProf includes a number of approaches to handle PSF convolution. The main concept is that AutoProf will convolve its model with a PSF before comparing against an image. The PSF behaviour of a model is determined by the *psf_mode* parameter which can be set before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first a psf is needed, this is stored with the target object\n",
    "# Here we simply construct a gaussian PSF image that is 31 pixels across\n",
    "# Note the PSF must always be odd in its dimensions\n",
    "xx, yy = np.meshgrid(np.linspace(-5,5,31), np.linspace(-5,5,31))\n",
    "PSF = np.exp(-(xx**2 + yy**2)/5**2)\n",
    "PSF /= np.sum(PSF)\n",
    "target = ap.image.Target_Image(\n",
    "    data = target_data,\n",
    "    pixelscale = 0.262,\n",
    "    zeropoint = 22.5,\n",
    "    psf = PSF,\n",
    ")\n",
    "\n",
    "model_nopsf = ap.models.AutoProf_Model(\n",
    "    name = \"model without psf\", \n",
    "    model_type = \"sersic galaxy model\",\n",
    "    target = target,\n",
    "    parameters = {\"center\": [90,90], \"q\": 0.6, \"PA\": 60*np.pi/180, \"n\": 2, \"Re\": 10, \"Ie\": 1},\n",
    "    psf_mode = \"none\", # no PSF convolution will be done\n",
    ")\n",
    "model_psf = ap.models.AutoProf_Model(\n",
    "    name = \"model with psf\", \n",
    "    model_type = \"sersic galaxy model\",\n",
    "    target = target,\n",
    "    parameters = {\"center\": [90,90], \"q\": 0.6, \"PA\": 60*np.pi/180, \"n\": 2, \"Re\": 10, \"Ie\": 1},\n",
    "    psf_mode = \"full\", # now the full window will be PSF convolved\n",
    ")\n",
    "print(\"psf mode: \", model_psf.psf_mode)\n",
    "\n",
    "# With a convolved sersic the center is much more smoothed out\n",
    "fig, ax = plt.subplots(1,2,figsize = (16,7))\n",
    "ap.plots.model_image(fig, ax[0], model_nopsf)\n",
    "ax[0].set_title(\"No PSF convolution\")\n",
    "ap.plots.model_image(fig, ax[1], model_psf)\n",
    "ax[1].set_title(\"With PSF convolution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic things to do with a model\n",
    "\n",
    "Now that we know how to create a model and fit it to an image, lets get to know the model a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "\n",
    "model2.save() # will default to save as AutoProf.yaml\n",
    "with open(\"AutoProf.yaml\", \"r\") as f:\n",
    "    print(f.read()) # show what the saved file looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model from a file\n",
    "\n",
    "# note that the target still must be specified, only the parameters are saved\n",
    "model4 = ap.models.AutoProf_Model(name = \"no name\", filename = \"AutoProf.yaml\", target = target)\n",
    "print(model4) # can see that it has been constructed with all the same parameters as the saved model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model image to a file\n",
    "\n",
    "model2().save(\"model2.fits\")\n",
    "\n",
    "saved_image_hdu = fits.open(\"model2.fits\")\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "ax.imshow(\n",
    "    np.log10(saved_image_hdu[0].data), \n",
    "    origin = \"lower\",\n",
    "    cmap = ap.plots.visuals.cmap_grad, # gradient colourmap default for AutoProf\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load a target image\n",
    "\n",
    "target.save(\"target.fits\")\n",
    "\n",
    "new_target = ap.image.Target_Image(filename = \"target.fits\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "ap.plots.target_image(fig, ax, new_target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the model new parameter values manually\n",
    "\n",
    "print(\"parameter input order: \", model4.parameter_order()) # use this to see what order you have to give the parameters as input\n",
    "\n",
    "# plot the old model\n",
    "fig9, ax9 = plt.subplots(1,2,figsize = (16,7))\n",
    "ap.plots.model_image(fig9, ax9[0], model4)\n",
    "T = ax9[0].set_title(\"parameters as loaded\")\n",
    "\n",
    "# update and plot the new parameters\n",
    "new_parameters = torch.tensor([75, 110, 0.4, 20*np.pi/180, 3, 25, 0.12]) # note that the center parameter needs two values as input\n",
    "model4.initialize() # initialize must be called before optimization, or any other activity in which parameters are updated\n",
    "model4.set_parameters(new_parameters) # full_sample will update the parameters, then run sample and return the model image \n",
    "ap.plots.model_image(fig9, ax9[1], model4)\n",
    "T = ax9[1].set_title(\"new parameter values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the model image pixels directly\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "pixels = model4().data.detach().cpu().numpy()# model4.model_image.data is the pytorch stored model image pixel values. Calling detach().cpu().numpy() is needed to get the data out of pytorch and in a usable form\n",
    "\n",
    "im = plt.imshow(\n",
    "    np.log10(pixels), # take log10 for better dynamic range\n",
    "    origin = \"lower\",\n",
    "    cmap = ap.plots.visuals.cmap_grad, # gradient colourmap default for AutoProf\n",
    ")\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models can be constructed by providing model_type, or by creating the desired class directly\n",
    "\n",
    "#                     notice this is no longer \"AutoProf_Model\"\n",
    "model1_v2 = ap.models.Sersic_Galaxy(\n",
    "    name = \"model1 v2\",\n",
    "    parameters = {\"center\": [50,50], \"q\": 0.6, \"PA\": 60*np.pi/180, \"n\": 2, \"Re\": 10, \"Ie\": 1},\n",
    "    target = ap.image.Target_Image(np.zeros((100,100)), pixelscale = 1),\n",
    ")\n",
    "\n",
    "# This will be the same as model1\n",
    "print(model1_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the available model names\n",
    "\n",
    "# AutoProf keeps track of all the subclasses of the AutoProf_Model object, this list will \n",
    "# include all models even ones added by the user\n",
    "print(ap.models.AutoProf_Model.List_Model_Names(useable = True)) # set useable = None for all models, or useable = False for only base classes\n",
    "print(\"---------------------------\")\n",
    "# It is also possible to get all sub models of a specific Type\n",
    "print(\"only star models: \", ap.models.Star_Model.List_Model_Names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPU acceleration\n",
    "\n",
    "This one is easy! If you have a cuda enabled GPU available, AutoProf will just automatically detect it and use that device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if AutoProf has detected your GPU\n",
    "print(ap.AP_config.ap_device) # most likely this will say \"cpu\" unless you already have a cuda GPU, \n",
    "# in which case it should say \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a GPU but want to use the cpu for some reason, just set:\n",
    "ap.AP_config.ap_device = \"cpu\"\n",
    "# BEFORE creating anything else (models, images, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boost GPU acceleration with single precision float32\n",
    "\n",
    "If you are using a GPU you can get significant performance increases in both memory and speed by switching from double precision (the AutoProf default) to single precision floating point numbers. The trade off is reduced precision, this can cause some unexpected behaviors. For example an optimizer may keep iterating forever if it is trying to optimize down to a precision below what the float32 will track. Typically, numbers with float32 are good down to 6 places and AutoProf by default only attempts to minimize the Chi^2 to 3 places. However, to ensure the fit is secure to 3 places it often checks what is happenening down at 4 or 5 places. Hence, issues can arise. For the most part you can go ahead with float32 and if you run into a weird bug, try on float64 before looking further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again do this BEFORE creating anything else\n",
    "ap.AP_config.ap_dtype = torch.float32\n",
    "\n",
    "# Now new AutoProf objects will be made with single bit precision\n",
    "W1 = ap.image.Window(origin = [0,0], shape = [1,1])\n",
    "print(\"now a single:\", W1.origin.dtype)\n",
    "\n",
    "# Here we switch back to double precision\n",
    "ap.AP_config.ap_dtype = torch.float64\n",
    "W2 = ap.image.Window(origin = [0,0], shape = [1,1])\n",
    "print(\"back to double:\", W2.origin.dtype)\n",
    "print(\"old window is still single:\", W1.origin.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the window created as a float32 stays that way? That's really bad to have lying around! Make sure to change the data type before creating anything! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking output\n",
    "\n",
    "The AutoProf optimizers, and ocasionally the other AutoProf objects, will provide status updates about themselves which can be very useful for debugging problems or just keeping tabs on progress. There are a number of use cases for AutoProf, each having different desired output behaviors. To accomodate all users, AutoProf implements a general logging system. The object `ap.AP_config.ap_logger` is a logging object which by default writes to AutoProf.log in the local directory. As the user, you can set that logger to be any logging object you like for arbitrary complexity. Most users will, however, simply want to control the filename, or have it output to screen instead of a file. Below you can see examples of how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the log file will be where these tutorial notebooks are in your filesystem\n",
    "\n",
    "# Here we change the settings so AutoProf only prints to a log file\n",
    "ap.AP_config.set_logging_output(stdout = False, filename = \"AutoProf.log\")\n",
    "ap.AP_config.ap_logger.info(\"message 1: this should only appear in the AutoProf log file\")\n",
    "\n",
    "# Here we change the settings so AutoProf only prints to console\n",
    "ap.AP_config.set_logging_output(stdout = True, filename = None)\n",
    "ap.AP_config.ap_logger.info(\"message 2: this should only print to the console\")\n",
    "\n",
    "# Here we change the settings so AutoProf prints to both, which is the default\n",
    "ap.AP_config.set_logging_output(stdout = True, filename = \"AutoProf.log\")\n",
    "ap.AP_config.ap_logger.info(\"message 3: this should appear in both the console and the log file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also change the logging level and/or formatter for the stdout and filename options (see `help(ap.AP_config.set_logging_output)` for details). However, at that point you may want to simply make your own logger object and assign it to the `ap.AP_config.ap_logger` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
