{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Methods\n",
    "\n",
    "Here we will explore the various fitting methods in AutoProf. You have already encountered some of the methods, but here we will take a more systematic approach and discuss their strengths/weaknesses. Each method will be applied to the same problem with the same initial conditions so you can see how they operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde as kde\n",
    "\n",
    "%matplotlib inline\n",
    "import autoprof as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a fitting problem. You can ignore this cell to start, it just makes some test data to fit\n",
    "\n",
    "def true_params():\n",
    "\n",
    "    sky_param = np.array([1.5])\n",
    "    sersic_params = np.array([\n",
    "        [ 68.44035491,  65.58516735,   0.54945988, 127.19794926*np.pi/180,   2.14513004,   22.05219055,   2.45583024],\n",
    "        [ 54.00353786,  41.54430634,   0.40203928,  82.03862521*np.pi/180,   2.88613347,   12.095631,     2.76711163],\n",
    "        [ 43.13601431,  58.3422508,    0.71894728, 167.07973506*np.pi/180,   3.964371,     5.3767236,     2.41520244],\n",
    "    ])\n",
    "\n",
    "    return sersic_params, sky_param\n",
    "\n",
    "def init_params():\n",
    "\n",
    "    sky_param = np.array([1.4])\n",
    "    sersic_params = np.array([\n",
    "        [ 67.,  66.,   0.6, 130.*np.pi/180,   1.5,   25.,   2.],\n",
    "        [ 55.,  40.,   0.5,  80.*np.pi/180,   2.,   10.,     3.],\n",
    "        [ 41.,  60.,    0.8, 170.*np.pi/180,   3.,     4.,     2.],\n",
    "    ])\n",
    "\n",
    "    return sersic_params, sky_param\n",
    "\n",
    "def initialize_model(target, use_true_params = True):\n",
    "\n",
    "    # Pick parameters to start the model with\n",
    "    if use_true_params:\n",
    "        sersic_params, sky_param = true_params()\n",
    "    else:\n",
    "        sersic_params, sky_param = init_params()\n",
    "\n",
    "    # List of models, starting with the sky\n",
    "    model_list = [ap.models.AutoProf_Model(\n",
    "        name = \"sky\",\n",
    "        model_type = \"flat sky model\",\n",
    "        target = target,\n",
    "        parameters = {\"sky\": sky_param[0]},\n",
    "    )]\n",
    "    # Add models to the list\n",
    "    for i, params in enumerate(sersic_params):\n",
    "        model_list.append([\n",
    "            ap.models.AutoProf_Model(\n",
    "                name = f\"sersic {i}\",\n",
    "                model_type = \"sersic galaxy model\",\n",
    "                target = target,\n",
    "                parameters = {\n",
    "                    \"center\": [params[0],params[1]],\n",
    "                    \"q\": params[2],\n",
    "                    \"PA\": params[3],\n",
    "                    \"n\": params[4],\n",
    "                    \"Re\": params[5],\n",
    "                    \"Ie\": params[6],\n",
    "                },\n",
    "                #psf_mode = \"full\", # uncomment to try everything with PSF blurring (takes longer)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    MODEL = ap.models.Group_Model(\n",
    "        name = \"group\",\n",
    "        model_list = model_list,\n",
    "        target = target,\n",
    "    )\n",
    "    # Make sure every model is ready to go\n",
    "    MODEL.initialize()\n",
    "    \n",
    "    return MODEL\n",
    "\n",
    "def generate_target():\n",
    "\n",
    "    N = 100\n",
    "    pixelscale = 1.\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # PSF has sigma of 2x pixelscale\n",
    "    PSF = ap.utils.initialize.gaussian_psf(2, 21, pixelscale)\n",
    "    PSF /= np.sum(PSF)\n",
    "\n",
    "    target = ap.image.Target_Image(\n",
    "        data = np.zeros((N,N)),\n",
    "        pixelscale = pixelscale,\n",
    "        psf = PSF,\n",
    "    )\n",
    "\n",
    "    MODEL = initialize_model(target, True)\n",
    "    \n",
    "    # Sample the model with the true values to make a mock image\n",
    "    img = MODEL().data.detach().cpu().numpy()\n",
    "    # Add poisson noise\n",
    "    target.data = torch.Tensor(img + rng.normal(scale = np.sqrt(img)/2))  \n",
    "    target.variance = torch.Tensor(img/4)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    ap.plots.target_image(fig, ax, target)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def corner_plot(chain, labels=None, bins=50, true_values=None, plot_density=True, plot_contours=True, figsize=(10, 10)):\n",
    "    ndim = chain.shape[1]\n",
    "    \n",
    "    fig, axes = plt.subplots(ndim, ndim, figsize=figsize)\n",
    "    \n",
    "    for i in range(ndim):\n",
    "        for j in range(ndim):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                # Plot the histogram of parameter i\n",
    "                ax.hist(chain[:, i], bins=bins, histtype=\"step\", density=True, color=\"k\", lw=1)\n",
    "                \n",
    "                if plot_density:\n",
    "                    # Plot the kernel density estimate\n",
    "                    kde_x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)\n",
    "                    kde_y = kde(chain[:, i])(kde_x)\n",
    "                    ax.plot(kde_x, kde_y, color=\"green\", lw=1)\n",
    "                    \n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[i], color='red', linestyle='--', lw=1)\n",
    "\n",
    "            elif i > j:\n",
    "                # Plot the 2D histogram of parameters i and j\n",
    "                ax.hist2d(chain[:, j], chain[:, i], bins=bins, cmap=\"Greys\")\n",
    "\n",
    "                if plot_contours:\n",
    "                    # Plot the kernel density estimate contours\n",
    "                    kde_ij = kde([chain[:, j], chain[:, i]])\n",
    "                    x, y = np.mgrid[ax.get_xlim()[0]:ax.get_xlim()[1]:100j, ax.get_ylim()[0]:ax.get_ylim()[1]:100j]\n",
    "                    positions = np.vstack([x.ravel(), y.ravel()])\n",
    "                    kde_pos = np.reshape(kde_ij(positions).T, x.shape)\n",
    "                    ax.contour(x, y, kde_pos, colors=\"green\", linewidths=1, levels=5)\n",
    "                    \n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[j], color='red', linestyle='--', lw=1)\n",
    "                    ax.axhline(true_values[i], color='red', linestyle='--', lw=1)\n",
    "\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            if j == 0 and labels is not None:\n",
    "                ax.set_ylabel(labels[i])\n",
    "            else:\n",
    "                ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "\n",
    "            if i == ndim - 1 and labels is not None:\n",
    "                ax.set_xlabel(labels[j])\n",
    "            else:\n",
    "                ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "target = generate_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt\n",
    "\n",
    "This fitter is identitied as `ap.fit.LM` and it employs a variant of the second order Newton's method to converge very quickly to the local minimum. This is the generally accepted best algorithm for most use cases in Chi^2 minimization. If you don't know what to pick, start with this minimizer. The LM optimizer bridges the gap between first-order gradient descent and second order Newton's method. When far from the minimum, Newton's method is unstable and can give wildly wrong results, however, near the minimum it has \"quadratic convergence.\" This means that once near the minimum it takes only a few iterations to converge to several decimal places. The \"L\" scale parameter goes from L >> 1 which represents gradient descent to L << 1 which is Newton's Method. \n",
    "\n",
    "LM can handle a lot of scenarios and converge to the minimum. Keep in mind, however, that it is seeking a local minimum, so it is best to start off the algorithm as close as possible to the best fit parameters. AutoProf can automatically initialize, as discussed in other notebooks, but even that needs help sometimes (often in the form of a segmentation map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1,4, figsize = (24,5))\n",
    "plt.subplots_adjust(wspace= 0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res = ap.fit.LM(MODEL, verbose = 1).fit()\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Fit (models)\n",
    "\n",
    "An iterative fitter is identified as ap.fit.Iter and this makes use of the other fitters under certain circumstances. This method is generally employed for large models where it is not feasible to hold all the relevant data in memory at once. The iterative fitter will cycle through the models in a `Group_Model` object and fit them one at a time to the image, using the residuals from the previous cycle. This can be a very robust way to deal with some fits, especially if the overlap between models is not too strong. It is however more dependent on good initialization than other methods like the Levenberg-Marquardt. Also, it is possible for the Iter method to get stuck under certaint circumstances.\n",
    "\n",
    "Note that while the Iterative fitter needs a `Group_Model` object to iterate over, it is not necessarily true that the sub models are `Component_Model` objects, they could be `Group_Model` objects as well. In this way it is possible to cycle through and fit \"clusters\" of objects that are nearby, so long as it doesn't consume too much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1,4, figsize = (24,5))\n",
    "plt.subplots_adjust(wspace= 0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res = ap.fit.Iter(MODEL, verbose = 1).fit()\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Fit (parameters)\n",
    "\n",
    "This is an iterative fitter identified as ap.fit.Iter_LM and method is generally employed for large models where it is not feasible to hold all the relevant data in memory at once. The iterative fitter will cycle through chunks of parameters (you can choose how many parameters at a time) and fit them one at a time to the image, using the residuals from the previous cycle. This can be a very robust way to deal with some fits, especially if the overlap between models is not too strong. \n",
    "\n",
    "Note that this is iterating over the parameters, not the models. This allows it to handle parameter covariances even for very large models (if they happen to land in the same chunk). However, for this to work it must evaluate the whole model at each iteration making it somewhat slower in principle than the regular Iter fitter, though it can make up for it by fitting larger chunks at a time which makes the whole optimization faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1,4, figsize = (24,5))\n",
    "plt.subplots_adjust(wspace= 0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res = ap.fit.Iter_LM(MODEL, chunks = 11, verbose = 1).fit()\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "A gradient descent fitter is identified as ap.fit.Grad and uses standard first order derivative methods as provided by pytorch. These gradient descent methods include Adam, classic plus momentum, and LBFGS to name a few. The first order gradient is faster to evaluate and uses less memory, however it is considerably slower to converge than Levenberg-Marquardt. The gradient descent method with a small learning rate will reliably converge towards a local minimum, it will just do so slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1,4, figsize = (24,5))\n",
    "plt.subplots_adjust(wspace= 0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "\n",
    "res = ap.fit.Grad(MODEL, max_iter = 500, verbose = 1, optim_kwargs = {\"lr\": 5e-3}).fit()\n",
    "\n",
    "ap.plots.model_image(fig, axarr[2], MODEL)\n",
    "axarr[2].set_title(\"Model after optimization\")\n",
    "ap.plots.residual_image(fig, axarr[3], MODEL)\n",
    "axarr[3].set_title(\"Residuals after optimization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No U-Turn Sampler (NUTS)\n",
    "\n",
    "Unlike the above methods, NUTS does not stricktly seek a minimum chi^2, instead it is an MCMC method which seeks to explore the likelihood space and provide a full posterior in the form of random samples. The NUTS method in AutoProf is actually just a wrapper for the pyro implementation (__[link here](https://docs.pyro.ai/en/stable/index.html)__). Most of the functionality can be accessed this way, though for very advanced applications it may be necessary to manually interface with pyro (this is not very challenging as AutoProf is fully differentiable).\n",
    "\n",
    "The first iteration of NUTS is always very slow since it compiles the forward method on the fly, after that each sample is drawn very quickly. The warmup iterations take longer as the method is exploring the space and determining the ideal step size for fast integration with minimal numerical error. Once the algorithm begins sampling it is able to move quickly (for an MCMC) throught the parameter space. For many models, the NUTS sampler is able to collect nearly completely uncorrelated samples, meaning that even 100 is enough to get a good estimate of the posterior sometimes.\n",
    "\n",
    "NUTS is far faster than other MCMC implementations such as a standard Metropolis Hastings MCMC. However, it is still a lot slower than the other optimizers (LM) since it is doing more than seeking a single high likelihood point, it is fully exploring the likelihood space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = initialize_model(target, False)\n",
    "fig, axarr = plt.subplots(1,2, figsize = (12,5))\n",
    "plt.subplots_adjust(wspace= 0.1)\n",
    "ap.plots.model_image(fig, axarr[0], MODEL)\n",
    "axarr[0].set_title(\"Model before optimization\")\n",
    "ap.plots.residual_image(fig, axarr[1], MODEL)\n",
    "axarr[1].set_title(\"Residuals before optimization\")\n",
    "plt.show()\n",
    "\n",
    "# Use LM to start the sampler at a high likelihood location, no burn-in needed!\n",
    "res1 = ap.fit.LM(MODEL).fit()\n",
    "\n",
    "# Run the NUTS sampler\n",
    "res = ap.fit.NUTS(MODEL, warmup = 20, max_iter = 100).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corner plot of the posterior\n",
    "param_names = list(MODEL.parameter_order())\n",
    "i = 0\n",
    "while i < len(param_names):\n",
    "    param_names[i] = param_names[i].replace(\" \", \"\")\n",
    "    if \"center\" in param_names[i]:\n",
    "        center_name = param_names.pop(i)\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"y\"))\n",
    "        param_names.insert(i, center_name.replace(\"center\", \"x\"))\n",
    "    i += 1\n",
    "    \n",
    "ser, sky = true_params()\n",
    "corner_plot(\n",
    "    res.chain.detach().cpu().numpy(), \n",
    "    labels = param_names, \n",
    "    figsize = (20,20), \n",
    "    true_values = np.concatenate((sky,ser.ravel()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
